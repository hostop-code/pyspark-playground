version: '3.8'

networks:
  hadoop-network:
    external: true

# volumes:
#   namenode_data:
#   datanode_data:
    
services:
  hdfs-namenode:
    container_name: hdfs-namenode
    hostname: hdfs-namenode
    build:
      dockerfile: Dockerfile
      context: ./hadoop/
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'namenode']
    ports:
      - '9000:9000'
      - '9870:9870'
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:9870"]
      interval: 10s
      timeout: 5s
      retries: 3

  hdfs-secondarynamenode:
    container_name: hdfs-secondarynamenode
    hostname: hdfs-secondarynamenode
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'secondarynamenode']
    ports:
      - '50090:50090'
    depends_on:
      - hdfs-namenode
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '2'  # Set the desired CPU limit (e.g., half of a core)
          memory: '2G'
    healthcheck:
      test: ["CMD-SHELL", "(lsof -i :50090 && exit 0) || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 3

  hdfs-datanode:
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'datanode']
    expose:
      - 9864
    depends_on:
      - hdfs-namenode
      - hdfs-secondarynamenode
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2'  # Set the desired CPU limit
          memory: '2G'
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:9864"]
      interval: 10s
      timeout: 5s
      retries: 3

  postgresdb:
    hostname: hivemetastore
    build:
          dockerfile: Dockerfile
          context: ./postgres/
    image: kevinity310/postgres-hms:11.5
    container_name: postgresdb
    environment:
      POSTGRES_PASSWORD: postgres
    expose:
      - 5432
    # volumes:
    #   - postgres_hms_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 15s
      timeout: 5s
      retries: 5
    networks:
      - hadoop-network

  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    build:
      dockerfile: Dockerfile
      context: ./hive/
    image: kevinity310/hive:3.1.3
    entrypoint: ['./entrypoint.sh', 'hive-metastore']
    depends_on:
      - postgresdb
      - hdfs-namenode
      - hdfs-secondarynamenode
    expose:
      - 9083
    ports:
      - '9083:9083' # Hive Metastore service
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '2'  # Set the desired CPU limit (e.g., half of a core)
          memory: '2G'

  spark-master:
    container_name: "spark-master"
    hostname: spark-master
    build:
      dockerfile: Dockerfile
      context: ./spark/standalone/
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-master']
    ports:
      - '7077:7077' # Spark Master
      - '8081:8080' # Spark Master UI
    depends_on:
      - hdfs-namenode
      - hdfs-secondarynamenode
      - hive-metastore
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 3

  spark-history-server:
    container_name: "spark-history-server"
    hostname: spark-history-server
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-history-server']
    depends_on:
      - spark-master
    ports:
      - '18080:18080'
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:18080"]
      interval: 10s
      timeout: 5s
      retries: 3
      
  spark-worker:
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-worker']
    depends_on:
      - spark-master
    networks:
      - hadoop-network
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '2'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit

  trino-coordinator:
    container_name: "trino-coordinator"
    hostname: trino-coordinator
    build:
      dockerfile: Dockerfile
      context: ./trino/trino-coordinator
    image: "kevinity310/trino-coordinator:435"
    ports:
      - "8080:8080"
    command: 
      - "http://trino-coordinator:8080"
      - "coordinator"
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 3

  trino-worker:
    build:
      dockerfile: Dockerfile
      context: ./trino/trino-worker
    image: "kevinity310/trino-worker:435"
    command: 
      - "http://trino-coordinator:8080"
    depends_on:
      - trino-coordinator
    expose:
      - 8080
    networks:
      - hadoop-network
    deploy:
      replicas: 1  # Set the desired number of replicas
      resources:
        limits:
          cpus: '2'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 
    healthcheck:
      test: 
        - "CMD-SHELL"
        - "netstat -an | grep LISTEN | grep 8080"
      interval: 10s
      timeout: 5s
      retries: 3

  jupyterhub:
    container_name: "jupyterhub"
    hostname: jupyterhub
    build:
      dockerfile: Dockerfile
      context: ./jupyterhub/
    image: kevinity310/jupyterhub:4.0.2 
    entrypoint: ['./entrypoint.sh', 'jupyterhub']
    ports:
      - '8181:8000'
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '2'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 
      
# docker compose -f docker-compose-dev.yml build
# docker compose -f docker-compose-dev.yml up -d

# Create network :
# docker network create --driver=overlay hadoop-network

  



