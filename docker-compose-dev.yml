version: '3.8'

networks:
  hadoop-network:
    external: true

# volumes:
#   namenode_data:
#   datanode_data:
    
services:
  hdfs-namenode:
    container_name: hdfs-namenode
    hostname: hdfs-namenode
    build:
      dockerfile: Dockerfile
      context: ./hadoop/
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'namenode']
      # - namenode_data:/opt/hadoop/data/dataNode
    ports:
      - '9000:9000'
      - '9870:9870'
    networks:
      - hadoop-network

  hdfs-secondarynamenode:
    container_name: hdfs-secondarynamenode
    hostname: hdfs-secondarynamenode
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'secondarynamenode']
    ports:
      - '50090:50090'
    depends_on:
      - hdfs-namenode
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit (e.g., half of a core)
          memory: '6G'

  hdfs-datanode:
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'datanode']
    expose:
      - 9864
    depends_on:
      - hdfs-namenode
    # volumes:
    #   - ./book_data:/opt/spark/data
    #   - ./spark_apps:/opt/spark/apps
    # Gak bisa hdfs punya id unic, dipakai nodename dimana metadata data disimpan (kalau restart gpp, tapi build gak bisa)
    # docker-compose up --build --renew-anon-volumes
    # volumes:
    #   - datanode_data:/opt/hadoop/data/dataNode
    deploy:
      resources:
        limits:
          cpus: '2'  # Set the desired CPU limit
          memory: '4G'
    networks:
      - hadoop-network
  
  # yarn-resource-manager:
  #   container_name: yarn-resource-manager
  #   image: kevinity310/hadoop-base:3.2.4
  #   entrypoint: ['./entrypoint.sh', 'yarn-resource-manager']
  #   ports:
  #     - '8030:8030'
  #     - '8031:8031'
  #     - '8032:8032'
  #     - '8033:8033'
  #     - '8088:8088'
  #   networks:
  #     - hadoop-network
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '4'  # Set the desired CPU limit
  #         memory: '4G'

  # yarn-node-manager:
  #   container_name: yarn-node-manager
  #   image: kevinity310/hadoop-base:3.2.4
  #   entrypoint: ['./entrypoint.sh', 'yarn-node-manager']
  #   networks:
  #     - hadoop-network
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '4'  # Set the desired CPU limit 
  #         memory: '4G'  # Set the desired memory limit 

# HIVE Development:
  postgresdb:
    hostname: hivemetastore
    build:
          dockerfile: Dockerfile
          context: ./postgres/
    image: kevinity310/postgres-hms:11.5
    container_name: postgresdb
    environment:
      POSTGRES_PASSWORD: postgres
    expose:
      - 5432
    # volumes:
    #   - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - hadoop-network
  
  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    build:
      dockerfile: Dockerfile
      context: ./hive/
    image: kevinity310/hive:3.1.3
    entrypoint: ['./entrypoint.sh', 'hive-metastore']
    depends_on:
      - postgresdb
      - hdfs-namenode
    expose:
      - 9083
    ports:
      - '9083:9083' # Hive Metastore service
    networks:
      - hadoop-network

  hive-server2:
    container_name: hive-server2
    hostname: hive-server2
    image: kevinity310/hive:3.1.3
    entrypoint: ['./entrypoint.sh', 'hive-server2']
    depends_on:
      - hive-metastore
    ports:
      - '10000:10000' # Hive Metastore Database 
      - '10002:10002' # Hive UI
    networks:
      - hadoop-network

  trino-coordinator:
    container_name: "trino-coordinator"
    hostname: trino-coordinator
    build:
      dockerfile: Dockerfile
      context: ./trino/trino-coordinator
    image: "kevinity310/trino-coordinator:435"
    ports:
      - "8081:8080"
    command: 
      - "http://trino-coordinator:8080"
      - "coordinator"
    networks:
      - hadoop-network

  trino-worker:
    build:
      dockerfile: Dockerfile
      context: ./trino/trino-worker
    image: "kevinity310/trino-worker:435"
    command: 
      - "http://trino-coordinator:8080"
    depends_on:
      - trino-coordinator
    expose:
      - 8080
    networks:
      - hadoop-network
    deploy:
      replicas: 2  # Set the desired number of replicas
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 
      
  # docker compose -f docker-compose-dev.yml up  -scale trino-worker=2  -d
  
  spark-master:
    container_name: "spark-master"
    hostname: spark-master
    build:
      dockerfile: Dockerfile
      context: ./spark/standalone/
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-master']
    ports:
      - '7077:7077'
      - '8080:8080'
      - '4040:4040' 
    networks:
      - hadoop-network

  spark-history-server:
    container_name: "spark-history-server"
    hostname: spark-history-server
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-history-server']
    depends_on:
      - spark-master
    ports:
      - '18080:18080'
    networks:
      - hadoop-network
      
  spark-worker:
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-worker']
    depends_on:
      - spark-master
    # ports:
    #   - '8181:8081'
    networks:
      - hadoop-network
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 

  jupyterhub:
    container_name: "jupyterhub"
    hostname: jupyterhub
    build:
      dockerfile: Dockerfile
      context: ./jupyterhub/
    image: kevinity310/jupyterhub:latest
    entrypoint: ['./entrypoint.sh', 'jupyterhub']
    ports:
      - '8181:8000'
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 
  

# docker compose -f docker-compose-dev.yml build
# docker compose -f docker-compose-dev.yml up -d

  



