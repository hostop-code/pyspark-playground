version: '3.8'

networks:
  hadoop-network:
    external: true

# volumes:
#   namenode_data:
#   datanode_data:
    
services:
  hdfs-namenode:
    container_name: hdfs-namenode
    build:
      dockerfile: Dockerfile
      context: ./hadoop/
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'namenode']
      # - namenode_data:/opt/hadoop/data/dataNode
    ports:
      - '9000:9000'
      - '9870:9870'
    networks:
      - hadoop-network

  hdfs-secondarynamenode:
    container_name: hdfs-secondarynamenode
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'secondarynamenode']
    ports:
      - '50090:50090'
    depends_on:
      - hdfs-namenode
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit (e.g., half of a core)
          memory: '6G'

  hdfs-datanode:
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'datanode']
    expose:
      - 9864
    depends_on:
      - hdfs-namenode
    volumes:
      - ./book_data:/opt/spark/data
      - ./spark_apps:/opt/spark/apps
    # Gak bisa hdfs punya id unic, dipakai nodename dimana metadata data disimpan (kalau restart gpp, tapi build gak bisa)
    # docker-compose up --build --renew-anon-volumes
    # volumes:
    #   - datanode_data:/opt/hadoop/data/dataNode
    deploy:
      resources:
        limits:
          cpus: '2'  # Set the desired CPU limit
          memory: '4G'
    networks:
      - hadoop-network
  
  yarn-resource-manager:
    container_name: yarn-resource-manager
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'yarn-resource-manager']
    ports:
      - '8030:8030'
      - '8031:8031'
      - '8032:8032'
      - '8033:8033'
      - '8088:8088'
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit
          memory: '4G'

  yarn-node-manager:
    container_name: yarn-node-manager
    image: kevinity310/hadoop-base:3.2.4
    entrypoint: ['./entrypoint.sh', 'yarn-node-manager']
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 

# HIVE Development:
  postgresdb:
    image: postgres:11.5
    hostname: hivemetastore
    environment:
      POSTGRES_PASSWORD: postgres
    expose:
      - 5432
    volumes:
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - hadoop-network
  
  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    build:
      dockerfile: Dockerfile
      context: ./hive/
    image: kevinity310/hive:3.1.3
    entrypoint: ['./entrypoint.sh', 'hive-metastore']
    depends_on:
      - postgresdb
      - hdfs-namenode
      - yarn-resource-manager
    expose:
      - 9083
    ports:
      - '9083:9083' # Hive Metastore service
    networks:
      - hadoop-network

  hive-server2:
    container_name: hive-server2
    image: kevinity310/hive:3.1.3
    entrypoint: ['./entrypoint.sh', 'hive-server2']
    depends_on:
      - hive-metastore
    ports:
      - '10000:10000' # Hive Metastore Database 
      - '10002:10002' # Hive UI
    networks:
      - hadoop-network

# docker compose -f docker-compose-dev.yml build
# docker compose -f docker-compose-dev.yml up -d

  



