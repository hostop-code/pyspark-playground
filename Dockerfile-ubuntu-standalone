# Use the official Python image as the base image
FROM ubuntu:22.04 as linux-base

# Disable interactive installation
ARG DEBIAN_FRONTEND=noninteractive

ARG SPARK_VERSION=3.4.2
ARG HADOOP_VERSION=3.3.6
ARG ZEPPELIN_VERSION=0.10.1
ARG HIVE_VERSION=3.1.3

# Install tools required by the OS
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      software-properties-common \
      sudo \
      curl \
      iputils-ping \
      vim \
      unzip \
      rsync \
      build-essential \
      dos2unix \
      nano \
      python3 \
    python3-pip \
    openssh-client \
    && apt-get clean 
    
# Print a message during the build process
RUN echo "INSTALING JAVA version 1.8!"
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      openjdk-11-jdk \
      maven \
  && apt-get clean 
    # && rm -rf /var/lib/apt/lists/*

RUN echo "INSTALING wget & net-toolsðŸ« "
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      wget \
    net-tools \
  && apt-get clean 

FROM linux-base as hadoop-base

# Setup the directories for Hadoop Installation
ENV HADOOP_HOME=/opt/hadoop

RUN mkdir -p $HADOOP_HOME

WORKDIR $HADOOP_HOME

# Download and install Hadoop
RUN curl -L https://dlcdn.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -o hadoop-${HADOOP_VERSION}.tar.gz \
 && tar xfz hadoop-${HADOOP_VERSION}.tar.gz --directory ${HADOOP_HOME} --strip-components 1 \
 && rm -rf hadoop-${HADOOP_VERSION}.tar.gz

FROM hadoop-base as spark-base 

ENV SPARK_HOME=/opt/spark
RUN mkdir -p $SPARK_HOME
WORKDIR $SPARK_HOME
 
# Download and install Spark
RUN curl -L https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz -o spark-${SPARK_VERSION}-bin-hadoop3.tgz \
 && tar xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory ${SPARK_HOME} --strip-components 1 \
 && rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz
 
FROM spark-base as zeppelin_base

ENV ZEPPELIN_HOME=/opt/zeppelin

RUN mkdir -p $ZEPPELIN_HOME 
WORKDIR $ZEPPELIN_HOME
# Download and install apache Zeppelin
RUN curl -L https://dlcdn.apache.org/zeppelin/zeppelin-${ZEPPELIN_VERSION}/zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz -o zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz \
 && tar xfz zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz --directory ${ZEPPELIN_HOME} --strip-components 1 \
 && rm -rf zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz

FROM zeppelin_base as hive_base

ENV HIVE_HOME=/opt/hive
RUN mkdir -p $HIVE_HOME 

RUN curl -L https://dlcdn.apache.org/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz -o apache-hive-3.1.3-bin.tar.gz \
 && tar xvzf apache-hive-${HIVE_VERSION}-bin.tar.gz --directory ${HIVE_HOME} --strip-components 1 \
 && rm -rf apache-hive-${HIVE_VERSION}-bin.tar.gz

FROM hive_base as jupyter_base

ENV JUPYTER_HOME=/opt/jupyter 
RUN mkdir -p $JUPYTER_HOME
WORKDIR $JUPYTER_HOME

# INSTALL JUPYTER LAB
RUN pip3 install ipykernel jupyterlab

FROM jupyter_base as trino_base

ENV TRINO_HOME=/opt/trino 
RUN mkdir -p $TRINO_HOME
WORKDIR $TRINO_HOME

RUN curl -L https://repo1.maven.org/maven2/io/trino/trino-server/435/trino-server-435.tar.gz -o trino-server-435.tar.gz \
 && tar xvzf trino-server-435.tar.gz --directory ${TRINO_HOME} --strip-components 1 \
 && rm -rf trino-server-435.tar.gz

COPY trino/* /opt/trino/etc/

# Install Python dependencies
COPY requirements/requirements.txt .
RUN pip3 install -r requirements.txt

# Set environment variables
ENV JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
ENV PATH="$TRINO_HOME/bin:$ZEPPELIN_HOME/bin:$SPARK_HOME/sbin:/opt/spark/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:${PATH}"
ENV HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop"
ENV LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:${LD_LIBRARY_PATH}"

# Set users for HDFS and Yarn
ENV HDFS_NAMENODE_USER="root"
ENV HDFS_DATANODE_USER="root"
ENV HDFS_SECONDARYNAMENODE_USER="root"
ENV YARN_RESOURCEMANAGER_USER="root"
ENV YARN_NODEMANAGER_USER="root"

# Add JAVA_HOME to haddop-env.sh
RUN echo "export JAVA_HOME=${JAVA_HOME}" >> "$HADOOP_HOME/etc/hadoop/hadoop-env.sh"

# Copy configuration files
COPY spark/spark-defaults.conf $SPARK_HOME/conf/

COPY hadoop/*.xml $HADOOP_HOME/etc/hadoop/

COPY zeppelin/conf/* $ZEPPELIN_HOME/conf/

COPY hive/*.xml $HIVE_HOME/conf/

# Make binaries and scripts executable, set PYTHONPATH
RUN chmod u+x $SPARK_HOME/sbin/* && \
    chmod u+x $SPARK_HOME/bin/* && \
    chmod u+x $ZEPPELIN_HOME/bin/* 
    # chmod u+x $TRINO_HOME/* 

    # chmod u+x $HIVE_HOME/bin/* 

ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH

# Config In HIVE
# Set environment variables in hive-config.sh
RUN echo 'export HADOOP_HOME=$HADOOP_HOME' >> "${HIVE_HOME}/bin/hive-config.sh" \
    && echo 'export HADOOP_HEAPSIZE=${HADOOP_HEAPSIZE:-1024}' >> "${HIVE_HOME}/bin/hive-config.sh"

RUN mv /opt/hive/lib/guava-19.0.jar /opt/hive/lib/guava-19.0.jar.bak
RUN cp /opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /opt/hive/lib/

# Generate SSH keys and configure
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 600 ~/.ssh/authorized_keys

COPY ssh_config ~/.ssh/config

# Copy entrypoint script
COPY entrypoint-standalone.sh entrypoint.sh

RUN dos2unix entrypoint.sh && chmod +x entrypoint.sh

EXPOSE 22

# Set entry point
# ENTRYPOINT ["./entrypoint.sh"]
CMD ["./entrypoint.sh"]

# docker compose -f docker-compose-ubuntu-standalone.yml build 
# docker run --rm -it Dockerfile-ubuntu-standalone /bin/bash

# service postgresql status