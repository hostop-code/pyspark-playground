version: '3.8'

networks:
  hadoop-network:
    external: true

volumes:
  postgres_hms_data:
#   namenode_data:
#   datanode_data:
    
services:
  postgresdb:
    hostname: hivemetastore
    build:
          dockerfile: Dockerfile
          context: ./postgres/
    image: kevinity310/postgres-hms:11.5
    container_name: postgresdb
    environment:
      POSTGRES_PASSWORD: postgres
    expose:
      - 5432
    volumes:
      - postgres_hms_data:/var/lib/postgresql/data
    #   - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - hadoop-network
  
  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    build:
      dockerfile: Dockerfile
      context: ./hive/
    image: kevinity310/hive:3.1.3
    entrypoint: ['./entrypoint.sh', 'hive-metastore']
    depends_on:
      - postgresdb
      - hdfs-namenode
    expose:
      - 9083
    ports:
      - '9083:9083' # Hive Metastore service
    networks:
      - hadoop-network

  hive-server2:
    container_name: hive-server2
    hostname: hive-server2
    image: kevinity310/hive:3.1.3
    entrypoint: ['./entrypoint.sh', 'hive-server2']
    depends_on:
      - hive-metastore
    ports:
      - '10000:10000' # Hive Metastore Database 
      - '10002:10002' # Hive UI
    networks:
      - hadoop-network

  trino-coordinator:
    container_name: "trino-coordinator"
    hostname: trino-coordinator
    build:
      dockerfile: Dockerfile
      context: ./trino/trino-coordinator
    image: "kevinity310/trino-coordinator:435"
    ports:
      - "8081:8080"
    command: 
      - "http://trino-coordinator:8080"
      - "coordinator"
    networks:
      - hadoop-network

  trino-worker:
    build:
      dockerfile: Dockerfile
      context: ./trino/trino-worker
    image: "kevinity310/trino-worker:435"
    command: 
      - "http://trino-coordinator:8080"
    depends_on:
      - trino-coordinator
    expose:
      - 8080
    networks:
      - hadoop-network
    deploy:
      replicas: 2  # Set the desired number of replicas
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 
      
  # docker compose -f docker-compose-dev.yml up  -scale trino-worker=2  -d
  
  spark-master:
    container_name: "spark-master"
    hostname: spark-master
    build:
      dockerfile: Dockerfile
      context: ./spark/standalone/
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-master']
    ports:
      - '7077:7077'
      - '8080:8080'
      - '4040:4040' 
    networks:
      - hadoop-network

  spark-history-server:
    container_name: "spark-history-server"
    hostname: spark-history-server
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-history-server']
    depends_on:
      - spark-master
    ports:
      - '18080:18080'
    networks:
      - hadoop-network
      
  spark-worker:
    image: kevinity310/spark-standalone:3.4.2
    entrypoint: ['./entrypoint.sh', 'spark-worker']
    depends_on:
      - spark-master
    # ports:
    #   - '8181:8081'
    networks:
      - hadoop-network
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 

  jupyterhub:
    container_name: "jupyterhub"
    hostname: jupyterhub
    build:
      dockerfile: Dockerfile
      context: ./jupyterhub/
    image: kevinity310/jupyterhub:latest
    entrypoint: ['./entrypoint.sh', 'jupyterhub']
    ports:
      - '8181:8000'
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          cpus: '4'  # Set the desired CPU limit 
          memory: '4G'  # Set the desired memory limit 
  

# docker compose -f docker-compose-dev.yml build
# docker compose -f docker-compose-dev.yml up -d

  



