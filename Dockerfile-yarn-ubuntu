# Use the official Python image as the base image
FROM ubuntu:22.04 as spark-base

# Disable interactive installation
ARG DEBIAN_FRONTEND=noninteractive

ARG SPARK_VERSION=3.3.4
ARG HADOOP_VERSION=3.3.5

# Install tools required by the OS
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      sudo \
      curl \
      vim \
      unzip \
      rsync \
      openjdk-8-jdk \
      build-essential \
      software-properties-common \
      dos2unix \
      nano \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Setup the directories for Spark and Hadoop installations
ENV SPARK_HOME=/opt/spark
ENV HADOOP_HOME=/opt/hadoop

RUN mkdir -p $HADOOP_HOME && mkdir -p $SPARK_HOME
WORKDIR $SPARK_HOME

# Download and install Spark
RUN curl -L https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz -o spark-${SPARK_VERSION}-bin-hadoop3.tgz \
 && tar xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory ${SPARK_HOME} --strip-components 1 \
 && rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz

# Download and install Hadoop
RUN curl -L https://dlcdn.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -o hadoop-${HADOOP_VERSION}-bin.tar.gz \
 && tar xfz hadoop-${HADOOP_VERSION}-bin.tar.gz --directory ${HADOOP_HOME} --strip-components 1 \
 && rm -rf hadoop-${HADOOP_VERSION}-bin.tar.gz

# Create a new stage for the pyspark image
FROM spark-base as pyspark

# Install Python dependencies
COPY requirements/requirements.txt .
RUN apt-get update && apt-get install -y python3-pip \
 && pip3 install -r requirements.txt

# Set environment variables
ENV JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
ENV PATH="$SPARK_HOME/sbin:/opt/spark/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:${PATH}"
ENV HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop"
ENV LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:${LD_LIBRARY_PATH}"

# Set users for HDFS and Yarn
ENV HDFS_NAMENODE_USER="root"
ENV HDFS_DATANODE_USER="root"
ENV HDFS_SECONDARYNAMENODE_USER="root"
ENV YARN_RESOURCEMANAGER_USER="root"
ENV YARN_NODEMANAGER_USER="root"

# Add JAVA_HOME to haddop-env.sh
RUN echo "export JAVA_HOME=${JAVA_HOME}" >> "$HADOOP_HOME/etc/hadoop/hadoop-env.sh"

# Copy configuration files
COPY yarn/spark-defaults.conf $SPARK_HOME/conf/
COPY yarn/*.xml $HADOOP_HOME/etc/hadoop/

# Make binaries and scripts executable, set PYTHONPATH
RUN chmod u+x $SPARK_HOME/sbin/* && \
    chmod u+x $SPARK_HOME/bin/*

ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH

RUN apt-get update && apt-get install -y openssh-client && rm -rf /var/lib/apt/lists/*

# Generate SSH keys and configure
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 600 ~/.ssh/authorized_keys

COPY ssh_config ~/.ssh/config

# Copy entrypoint script
COPY entrypoint-yarn.sh entrypoint.sh

RUN dos2unix entrypoint.sh && chmod +x entrypoint.sh

EXPOSE 22

# Set entry point
# ENTRYPOINT ["./entrypoint.sh"]
CMD ["./entrypoint.sh"]

# docker compose -f docker-compose-ubuntu.yarn.yml build 
# docker run --rm -it da-spark-yarn-image-ubuntu /bin/bash