# Use the official Python image as the base image
FROM ubuntu:22.04 as linux-base

# Disable interactive installation
ARG DEBIAN_FRONTEND=noninteractive

ARG SPARK_VERSION=3.2.0
ARG HADOOP_VERSION=3.2.4
ARG ZEPPELIN_VERSION=0.10.1
ARG HIVE_VERSION=3.1.3

# Install tools required by the OS
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      software-properties-common \
      sudo \
      curl \
      vim \
      unzip \
      rsync \
      build-essential \
      dos2unix \
      nano \
      python3 \
    python3-pip \
    openssh-client \
    && apt-get clean 
    
# Print a message during the build process
RUN echo "INSTALING JAVA version 1.8!"
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      openjdk-8-jdk \
      maven \
  && apt-get clean 
    # && rm -rf /var/lib/apt/lists/*

RUN echo "INSTALING wget ðŸ« "
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      wget \
  && apt-get clean 

FROM linux-base as hadoop-base
# Setup the directories for Hadoop Installation
ENV HADOOP_HOME=/opt/hadoop

RUN mkdir -p $HADOOP_HOME

WORKDIR $HADOOP_HOME

# Download and install Hadoop
RUN curl -L https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -o hadoop-${HADOOP_VERSION}-bin.tar.gz \
 && tar xfz hadoop-${HADOOP_VERSION}-bin.tar.gz --directory ${HADOOP_HOME} --strip-components 1 \
 && rm -rf hadoop-${HADOOP_VERSION}-bin.tar.gz

FROM hadoop-base as spark-base 

ENV SPARK_HOME=/opt/spark
RUN mkdir -p $SPARK_HOME
WORKDIR $SPARK_HOME
 
# Download and install Spark
RUN curl -L https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.2.tgz -o spark-${SPARK_VERSION}-bin-hadoop3.2.tgz \
 && tar xvzf spark-${SPARK_VERSION}-bin-hadoop3.2.tgz --directory ${SPARK_HOME} --strip-components 1 \
 && rm -rf spark-${SPARK_VERSION}-bin-hadoop3.2.tgz

FROM spark-base as zeppelin_base

ENV ZEPPELIN_HOME=/opt/zeppelin

RUN mkdir -p $ZEPPELIN_HOME 
WORKDIR $ZEPPELIN_HOME
# Download and install apache Zeppelin
RUN curl -L https://dlcdn.apache.org/zeppelin/zeppelin-${ZEPPELIN_VERSION}/zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz -o zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz \
 && tar xfz zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz --directory ${ZEPPELIN_HOME} --strip-components 1 \
 && rm -rf zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz

FROM zeppelin_base as postgres_base
# installing postgres 
RUN echo "INSTALING POSTGRESQL"

RUN curl -fsSL https://www.postgresql.org/media/keys/ACCC4CF8.asc|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/postgresql.gpg

RUN echo "deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main" |sudo tee  /etc/apt/sources.list.d/pgdg.list

# RUN cat /etc/apt/sources.list.d/pgdg.list

RUN apt-get update \ 
    &&  apt-get install -y --no-install-recommends \
    postgresql-11 \
    postgresql-client-11 \
    postgresql-contrib-11

# Adjust PostgreSQL configuration so that remote connections to the
# database are possible.
RUN echo "host all  all    0.0.0.0/0  md5" >> /etc/postgresql/11/main/pg_hba.conf

# And add ``listen_addresses`` to ``/etc/postgresql/11/main/postgresql.conf``
RUN echo "listen_addresses='*'" >> /etc/postgresql/11/main/postgresql.conf

USER postgres 

# change userpass in database
RUN /etc/init.d/postgresql start && \
    psql --command "ALTER USER postgres WITH PASSWORD 'postgres';" && \
    psql --command "CREATE DATABASE hivemetastoredb OWNER postgres;" && \
    /etc/init.d/postgresql stop

USER root

FROM postgres_base as hive_base

ENV HIVE_HOME=/opt/hive
RUN mkdir -p $HIVE_HOME 

RUN curl -L https://dlcdn.apache.org/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz -o apache-hive-3.1.3-bin.tar.gz \
 && tar xvzf apache-hive-${HIVE_VERSION}-bin.tar.gz --directory ${HIVE_HOME} --strip-components 1 \
 && rm -rf apache-hive-${HIVE_VERSION}-bin.tar.gz

# Install Python dependencies
COPY requirements/requirements.txt .
RUN pip3 install -r requirements.txt

# Set environment variables
ENV JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
ENV PATH="$ZEPPELIN_HOME/bin:$SPARK_HOME/sbin:/opt/spark/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:${PATH}"
ENV HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop"
ENV LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:${LD_LIBRARY_PATH}"

# Set users for HDFS and Yarn
ENV HDFS_NAMENODE_USER="root"
ENV HDFS_DATANODE_USER="root"
ENV HDFS_SECONDARYNAMENODE_USER="root"
ENV YARN_RESOURCEMANAGER_USER="root"
ENV YARN_NODEMANAGER_USER="root"

# Add JAVA_HOME to haddop-env.sh
RUN echo "export JAVA_HOME=${JAVA_HOME}" >> "$HADOOP_HOME/etc/hadoop/hadoop-env.sh"

# Copy configuration files
COPY yarn/spark-defaults.conf $SPARK_HOME/conf/
COPY yarn/*.xml $HADOOP_HOME/etc/hadoop/
COPY zeppelin/conf/* $ZEPPELIN_HOME/conf/
COPY hive/*.xml $HIVE_HOME/conf/

# Make binaries and scripts executable, set PYTHONPATH
RUN chmod u+x $SPARK_HOME/sbin/* && \
    chmod u+x $SPARK_HOME/bin/* && \
    chmod u+x $ZEPPELIN_HOME/bin/* 
    # chmod u+x $HIVE_HOME/bin/* 

ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH


# Config In HIVE
# Set environment variables in hive-config.sh
RUN echo 'export HADOOP_HOME=$HADOOP_HOME' >> "${HIVE_HOME}/bin/hive-config.sh" \
    && echo 'export HADOOP_HEAPSIZE=${HADOOP_HEAPSIZE:-1024}' >> "${HIVE_HOME}/bin/hive-config.sh"

RUN mv /opt/hive/lib/guava-19.0.jar /opt/hive/lib/guava-19.0.jar.bak
RUN cp /opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /opt/hive/lib/


# Generate SSH keys and configure
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 600 ~/.ssh/authorized_keys

COPY ssh_config ~/.ssh/config

# Copy entrypoint script
COPY entrypoint-yarn-hive.sh entrypoint.sh

RUN dos2unix entrypoint.sh && chmod +x entrypoint.sh

EXPOSE 22

# Set entry point
# ENTRYPOINT ["./entrypoint.sh"]
CMD ["./entrypoint.sh"]

# docker compose -f docker-compose-ubuntu-hive.yarn.yml build 
# docker run --rm -it da-spark-yarn-image-ubuntu-hive /bin/bash

# service postgresql status